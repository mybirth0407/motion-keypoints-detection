{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as data_utils\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Connect your script to Neptune\n",
    "import neptune\n",
    "import neptune_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain\\n049-1-1-03-Z17_C-0000005.jpg\\n049-1-1-03-Z17_C-0000007.jpg\\n049-1-1-03-Z17_C-0000019.jpg\\n049-1-1-03-Z17_C-0000021.jpg\\n049-1-1-03-Z17_C-0000023.jpg\\n050-1-1-03-Z17_C-0000011.jpg\\n050-1-1-03-Z17_C-0000015.jpg\\n050-1-1-03-Z17_C-0000029.jpg\\n561-1-3-27-Z134_C-0000009.jpg\\n\\ntest\\n729-3-5-36-Z94_A-0000013.jpg\\n730-3-5-36-Z94_A-0000011.jpg\\n730-3-5-36-Z94_A-0000015.jpg\\n730-3-5-36-Z94_C-0000015.jpg\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prefix data directory\n",
    "prefix_dir = '.'\n",
    "\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "# to the ImageFolder structure\n",
    "train_dir = f'{prefix_dir}/data/cropped_train_imgs'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = 'resnet'\n",
    "model_ver = '18'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 48\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs and earlystop to train for\n",
    "num_epochs = 200\n",
    "\n",
    "num_splits = 10\n",
    "num_earlystop = 10\n",
    "\n",
    "# Input size for resize image\n",
    "input_w = 150\n",
    "input_h = 150\n",
    "\n",
    "# Learning rate for optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# Use K-folds\n",
    "use_kfolds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "https://ui.neptune.ai/mybirth0407/dacon-motion/e/DAC-37\n"
     ]
    }
   ],
   "source": [
    "neptune.init(project_qualified_name='mybirth0407/dacon-motion',\n",
    "             api_token=neptune_config.token)\n",
    "\n",
    "with open(f'{prefix_dir}/counter.txt', 'r+') as f:\n",
    "    content = f.read().strip()\n",
    "    counter = int(content) + 1\n",
    "    f.seek(0)\n",
    "    f.write(f'{counter}')\n",
    "    print(counter)\n",
    "\n",
    "# Create experiment\n",
    "neptune.create_experiment(f'{counter:2d} - {model_name}{model_ver}')\n",
    "\n",
    "neptune.log_metric('batch_size', batch_size)\n",
    "neptune.log_metric('num_epochs', num_epochs)\n",
    "neptune.log_metric('num_splits', num_splits)\n",
    "neptune.log_metric('num_ealrystop', num_earlystop)\n",
    "neptune.log_metric('input_width', input_w)\n",
    "neptune.log_metric('input_height', input_h)\n",
    "neptune.log_metric('learning_rate', learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>nose_x</th>\n",
       "      <th>nose_y</th>\n",
       "      <th>left_eye_x</th>\n",
       "      <th>left_eye_y</th>\n",
       "      <th>right_eye_x</th>\n",
       "      <th>right_eye_y</th>\n",
       "      <th>left_ear_x</th>\n",
       "      <th>left_ear_y</th>\n",
       "      <th>right_ear_x</th>\n",
       "      <th>...</th>\n",
       "      <th>right_palm_x</th>\n",
       "      <th>right_palm_y</th>\n",
       "      <th>spine2(back)_x</th>\n",
       "      <th>spine2(back)_y</th>\n",
       "      <th>spine1(waist)_x</th>\n",
       "      <th>spine1(waist)_y</th>\n",
       "      <th>left_instep_x</th>\n",
       "      <th>left_instep_y</th>\n",
       "      <th>right_instep_x</th>\n",
       "      <th>right_instep_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001-1-1-01-Z17_A-0000001.jpg</td>\n",
       "      <td>138.389631</td>\n",
       "      <td>44.757881</td>\n",
       "      <td>133.655294</td>\n",
       "      <td>29.820225</td>\n",
       "      <td>151.429507</td>\n",
       "      <td>34.484230</td>\n",
       "      <td>112.117796</td>\n",
       "      <td>38.890539</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>111.484230</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>118.515770</td>\n",
       "      <td>214.054730</td>\n",
       "      <td>90.578836</td>\n",
       "      <td>526.718013</td>\n",
       "      <td>155.204067</td>\n",
       "      <td>538.827465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001-1-1-01-Z17_A-0000003.jpg</td>\n",
       "      <td>144.850679</td>\n",
       "      <td>34.711494</td>\n",
       "      <td>133.608552</td>\n",
       "      <td>18.593690</td>\n",
       "      <td>150.242111</td>\n",
       "      <td>19.593690</td>\n",
       "      <td>116.422997</td>\n",
       "      <td>25.694815</td>\n",
       "      <td>140.593682</td>\n",
       "      <td>...</td>\n",
       "      <td>156.187380</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>121.953248</td>\n",
       "      <td>148.062706</td>\n",
       "      <td>133.766231</td>\n",
       "      <td>202.797029</td>\n",
       "      <td>77.265676</td>\n",
       "      <td>393.062706</td>\n",
       "      <td>141.376234</td>\n",
       "      <td>535.499445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001-1-1-01-Z17_A-0000005.jpg</td>\n",
       "      <td>150.475902</td>\n",
       "      <td>34.000008</td>\n",
       "      <td>144.717997</td>\n",
       "      <td>20.757889</td>\n",
       "      <td>161.648412</td>\n",
       "      <td>22.242119</td>\n",
       "      <td>127.039884</td>\n",
       "      <td>26.351571</td>\n",
       "      <td>152.461032</td>\n",
       "      <td>...</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>110.538960</td>\n",
       "      <td>139.054730</td>\n",
       "      <td>118.844144</td>\n",
       "      <td>192.890539</td>\n",
       "      <td>55.437847</td>\n",
       "      <td>505.757889</td>\n",
       "      <td>132.071417</td>\n",
       "      <td>538.749554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001-1-1-01-Z17_A-0000007.jpg</td>\n",
       "      <td>148.320047</td>\n",
       "      <td>49.452689</td>\n",
       "      <td>143.907194</td>\n",
       "      <td>32.117804</td>\n",
       "      <td>156.328382</td>\n",
       "      <td>41.913729</td>\n",
       "      <td>122.844144</td>\n",
       "      <td>28.913737</td>\n",
       "      <td>148.164191</td>\n",
       "      <td>...</td>\n",
       "      <td>163.406318</td>\n",
       "      <td>60.461040</td>\n",
       "      <td>88.937294</td>\n",
       "      <td>146.109462</td>\n",
       "      <td>96.375124</td>\n",
       "      <td>195.624866</td>\n",
       "      <td>107.305177</td>\n",
       "      <td>517.233767</td>\n",
       "      <td>265.516499</td>\n",
       "      <td>287.389997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001-1-1-01-Z17_A-0000009.jpg</td>\n",
       "      <td>146.046395</td>\n",
       "      <td>28.164191</td>\n",
       "      <td>134.717997</td>\n",
       "      <td>16.703163</td>\n",
       "      <td>146.132650</td>\n",
       "      <td>16.781079</td>\n",
       "      <td>119.258806</td>\n",
       "      <td>23.593690</td>\n",
       "      <td>137.812620</td>\n",
       "      <td>...</td>\n",
       "      <td>157.648429</td>\n",
       "      <td>19.109461</td>\n",
       "      <td>112.843791</td>\n",
       "      <td>138.687572</td>\n",
       "      <td>122.391088</td>\n",
       "      <td>195.843791</td>\n",
       "      <td>86.625231</td>\n",
       "      <td>490.218921</td>\n",
       "      <td>147.625956</td>\n",
       "      <td>524.765102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image      nose_x     nose_y  left_eye_x  \\\n",
       "0  001-1-1-01-Z17_A-0000001.jpg  138.389631  44.757881  133.655294   \n",
       "1  001-1-1-01-Z17_A-0000003.jpg  144.850679  34.711494  133.608552   \n",
       "2  001-1-1-01-Z17_A-0000005.jpg  150.475902  34.000008  144.717997   \n",
       "3  001-1-1-01-Z17_A-0000007.jpg  148.320047  49.452689  143.907194   \n",
       "4  001-1-1-01-Z17_A-0000009.jpg  146.046395  28.164191  134.717997   \n",
       "\n",
       "   left_eye_y  right_eye_x  right_eye_y  left_ear_x  left_ear_y  right_ear_x  \\\n",
       "0   29.820225   151.429507    34.484230  112.117796   38.890539   140.000000   \n",
       "1   18.593690   150.242111    19.593690  116.422997   25.694815   140.593682   \n",
       "2   20.757889   161.648412    22.242119  127.039884   26.351571   152.461032   \n",
       "3   32.117804   156.328382    41.913729  122.844144   28.913737   148.164191   \n",
       "4   16.703163   146.132650    16.781079  119.258806   23.593690   137.812620   \n",
       "\n",
       "   ...  right_palm_x  right_palm_y  spine2(back)_x  spine2(back)_y  \\\n",
       "0  ...    159.000000     35.000000      111.484230      155.000000   \n",
       "1  ...    156.187380     17.000000      121.953248      148.062706   \n",
       "2  ...    167.000000     31.000000      110.538960      139.054730   \n",
       "3  ...    163.406318     60.461040       88.937294      146.109462   \n",
       "4  ...    157.648429     19.109461      112.843791      138.687572   \n",
       "\n",
       "   spine1(waist)_x  spine1(waist)_y  left_instep_x  left_instep_y  \\\n",
       "0       118.515770       214.054730      90.578836     526.718013   \n",
       "1       133.766231       202.797029      77.265676     393.062706   \n",
       "2       118.844144       192.890539      55.437847     505.757889   \n",
       "3        96.375124       195.624866     107.305177     517.233767   \n",
       "4       122.391088       195.843791      86.625231     490.218921   \n",
       "\n",
       "   right_instep_x  right_instep_y  \n",
       "0      155.204067      538.827465  \n",
       "1      141.376234      535.499445  \n",
       "2      132.071417      538.749554  \n",
       "3      265.516499      287.389997  \n",
       "4      147.625956      524.765102  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{prefix_dir}/data/cropped_train_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = df.iloc[:, 0].to_numpy()\n",
    "motions = df.iloc[:, 1:]\n",
    "columns = motions.columns.to_list()[::2]\n",
    "class_labels = [label.replace('_x', '').replace('_y', '') for label in columns]\n",
    "keypoints = []\n",
    "for motion in motions.to_numpy():\n",
    "    a_keypoints = []\n",
    "    for i in range(0, motion.shape[0], 2):\n",
    "        a_keypoints.append((float(motion[i]), float(motion[i+1])))\n",
    "    keypoints.append(a_keypoints)\n",
    "keypoints = np.array(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, earlystop=0, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    earlystop_value = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    best_loss = 999999999\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_since = time.time()\n",
    "        if earlystop and earlystop_value >= earlystop:\n",
    "            break\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs.float(), labels.float())\n",
    "                        loss2 = criterion(aux_outputs.float(), labels.float())\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs.float(), labels.float())\n",
    "\n",
    "                    # for classification\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # for classification\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                # for regression\n",
    "                running_corrects += torch.sum(outputs == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            epoch_time_elapsed = time.time() - epoch_since\n",
    "            print('{} ({}) Loss: {:.4f} Acc: {:.4f} Elapsed time: {:.0f}m {:.0f}s'.format(\n",
    "                phase, len(dataloaders[phase].dataset), epoch_loss, epoch_acc, epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
    "            neptune.log_metric(f'{phase}_loss', epoch_loss)\n",
    "            neptune.log_metric(f'{phase}_acc', epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    earlystop_value = 0\n",
    "                else:\n",
    "                    earlystop_value += 1\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training and Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best validation Acc: {:4f}\\n'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, {'acc': val_acc_history, 'loss': val_loss_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=48, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, model_ver, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG13_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg13_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft = initialize_model(model_name, model_ver, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentation and normalization for training\n",
    "# # Just resize and normalization for validation\n",
    "\n",
    "A_transforms = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            A.Resize(input_h, input_w, always_apply=True),\n",
    "#             A.Resize(input_h, input_w, p=1),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "#             A.OneOf([A.HorizontalFlip(p=1),\n",
    "#                      A.RandomRotate90(p=1),\n",
    "#                      A.VerticalFlip(p=1)            \n",
    "#             ], p=0.5),\n",
    "#             A.OneOf([A.MotionBlur(p=1),\n",
    "#                      A.GaussNoise(p=1)                 \n",
    "#             ], p=0.5),\n",
    "            A.MotionBlur(p=0.3),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=True, angle_in_degrees=True)),\n",
    "    \n",
    "    'val':\n",
    "        A.Compose([\n",
    "            A.Resize(input_h, input_w, always_apply=True),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=True, angle_in_degrees=True)),\n",
    "    \n",
    "    'test':\n",
    "        A.Compose([\n",
    "            A.Resize(input_h, input_w, always_apply=True),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, keypoints, phase, class_labels=None, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.keypoints = keypoints\n",
    "        self.phase = phase\n",
    "        self.class_labels = class_labels\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        keypoints = self.keypoints[idx]\n",
    "    \n",
    "        if self.data_transforms:\n",
    "            augmented = self.data_transforms[self.phase](image=img, keypoints=keypoints, class_labels=self.class_labels)\n",
    "            img = augmented['image']\n",
    "            keypoints = augmented['keypoints']\n",
    "        keypoints = np.array(keypoints).flatten()\n",
    "\n",
    "        return img, keypoints\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gather the parameters to be optimized/updated in this run. If we are\n",
    "# #  finetuning we will be updating all parameters. However, if we are\n",
    "# #  doing feature extract method, we will only update the parameters\n",
    "# #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "# #  is True.\n",
    "# params_to_update = model_ft.parameters()\n",
    "# print(\"Params to learn:\")\n",
    "# if feature_extract:\n",
    "#     params_to_update = []\n",
    "#     for name,param in model_ft.named_parameters():\n",
    "#         if param.requires_grad == True:\n",
    "#             params_to_update.append(param)\n",
    "#             print(\"\\t\",name)\n",
    "# else:\n",
    "#     for name,param in model_ft.named_parameters():\n",
    "#         if param.requires_grad == True:\n",
    "#             print(\"\\t\",name)\n",
    "\n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.Adam(params_to_update, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-folds use: False\n",
      "Epoch 1/200\n",
      "----------\n",
      "train (3775) Loss: 1851.7512 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 2170.4478 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train (3775) Loss: 966.7560 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 669.3168 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train (3775) Loss: 639.6926 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 514.5819 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train (3775) Loss: 596.0945 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 517.7041 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train (3775) Loss: 563.7539 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 654.5701 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train (3775) Loss: 532.4304 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 457.4572 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train (3775) Loss: 488.1067 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 400.4812 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train (3775) Loss: 464.2521 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 381.5963 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train (3775) Loss: 436.2025 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 368.2787 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train (3775) Loss: 371.7337 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 288.3683 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train (3775) Loss: 317.5872 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 212.6273 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train (3775) Loss: 288.6391 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 197.0944 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train (3775) Loss: 254.5173 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 161.4253 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train (3775) Loss: 233.9404 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 149.1351 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train (3775) Loss: 224.7635 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 138.6967 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train (3775) Loss: 203.2196 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "val (420) Loss: 136.1759 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train (3775) Loss: 185.3077 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 126.8412 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train (3775) Loss: 177.3193 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 118.9984 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train (3775) Loss: 169.2357 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "val (420) Loss: 113.2089 Acc: 0.0000 Elapsed time: 0m 30s\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train (3775) Loss: 160.9918 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 119.3347 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train (3775) Loss: 152.9948 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 104.8482 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train (3775) Loss: 147.9263 Acc: 0.0000 Elapsed time: 0m 23s\n",
      "val (420) Loss: 99.8647 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train (3775) Loss: 139.1959 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 97.2842 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train (3775) Loss: 143.7971 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 87.8916 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train (3775) Loss: 130.9516 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "val (420) Loss: 81.7001 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train (3775) Loss: 124.8524 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 86.8873 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train (3775) Loss: 116.4997 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 80.7389 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train (3775) Loss: 113.1442 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 72.3742 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train (3775) Loss: 112.4068 Acc: 0.0000 Elapsed time: 0m 24s\n",
      "val (420) Loss: 63.7655 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train (3775) Loss: 101.1674 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 66.6268 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train (3775) Loss: 95.4185 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 54.5503 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train (3775) Loss: 92.9255 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "val (420) Loss: 60.3403 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train (3775) Loss: 92.5340 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 60.3464 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train (3775) Loss: 87.9828 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 49.5055 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train (3775) Loss: 82.8529 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 58.5070 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train (3775) Loss: 82.6356 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 46.2830 Acc: 0.0000 Elapsed time: 0m 29s\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train (3775) Loss: 78.3514 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 47.2905 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train (3775) Loss: 73.5617 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 43.5459 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train (3775) Loss: 75.0182 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 41.7071 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train (3775) Loss: 77.3778 Acc: 0.0000 Elapsed time: 0m 26s\n",
      "val (420) Loss: 52.8453 Acc: 0.0000 Elapsed time: 0m 28s\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train (3775) Loss: 69.1164 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 41.7998 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train (3775) Loss: 69.8519 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 40.0709 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train (3775) Loss: 69.9029 Acc: 0.0000 Elapsed time: 0m 25s\n",
      "val (420) Loss: 48.1451 Acc: 0.0000 Elapsed time: 0m 27s\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f'k-folds use: {use_kfolds}')\n",
    "\n",
    "full_since = time.time()\n",
    "\n",
    "if use_kfolds:\n",
    "    kf = KFold(num_splits, random_state=42, shuffle=True)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(imgs)):\n",
    "        print(f'{i+1}/{num_splits} folds iteration')\n",
    "        since = time.time()\n",
    "        X_train, X_val = imgs[train_index], imgs[val_index]\n",
    "        y_train, y_val = keypoints[train_index], keypoints[val_index]\n",
    "        train_data = Dataset(train_dir, X_train, y_train, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "        val_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "        train_loader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train and evaluate\n",
    "        model_ft, hists = train_model(\n",
    "            model_ft, dataloaders, criterion, optimizer_ft,\n",
    "            num_epochs=num_epochs, earlystop=num_earlystop, is_inception=(model_name==\"inception\"))\n",
    "        torch.save(model_ft.state_dict(), f'{prefix_dir}/local/baseline_{counter:2d}_{model_name}{model_ver}_{i+1}.pt')\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "else:\n",
    "    since = time.time()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(imgs, keypoints, test_size=1/num_splits, random_state=42)\n",
    "    train_data = Dataset(train_dir, X_train, y_train, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "    val_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "    train_loader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, hists = train_model(\n",
    "        model_ft, dataloaders, criterion, optimizer_ft,\n",
    "        num_epochs=num_epochs, earlystop=num_earlystop, is_inception=(model_name==\"inception\"))\n",
    "    torch.save(model_ft.state_dict(), f'{prefix_dir}/local/baseline_{counter:2d}_{model_name}{model_ver}_{i+1}.pt')\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "fulltime_elapsed = time.time() - full_since\n",
    "print('All process done!\\nElapsed time: {:.0f}m {:.0f}s\\n'.format(fulltime_elapsed // 60, fulltime_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), f'{prefix_dir}/local/baseline_{counter:2d}_{model_name}{model_ver}_fv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.load_state_dict(torch.load(f'{prefix_dir}/local/baseline_{counter:2d}_{model_name}{model_ver}_fv.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./data/test_imgs\"\n",
    "test_imgs = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, phase, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.phase = phase\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgs[idx]\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "\n",
    "        if self.data_transforms:\n",
    "            augmented = self.data_transforms[self.phase](image=img)\n",
    "            img = augmented['image']\n",
    "        return filename, img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "test_data = TestDataset(test_dir, test_imgs, data_transforms=A_transforms, phase='test')\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size=batch_size * 4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "files = []\n",
    "with torch.no_grad():\n",
    "    for filenames, inputs in test_loader:\n",
    "        predictions = list(model_ft(inputs.to(device)).cpu().numpy())\n",
    "        files.extend(filenames)\n",
    "        for prediction in predictions:\n",
    "            all_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= input_w / 1920\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= input_h / 1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(f'{prefix_dir}/data/sample_submission.csv')\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{prefix_dir}/submission_{counter:2d}_{model_name}{model_ver}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_90 (Python 3.7)",
   "language": "python",
   "name": "torch_90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
